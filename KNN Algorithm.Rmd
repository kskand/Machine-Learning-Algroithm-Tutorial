---
title: "kNN(k-Nearest Neighbour) Algorithm"
author: "Kumar Skand"
date: "October 2, 2017"
output: html_document
Refrence: Various study material available online.
---

###What is kNN ?
Let's assume we have several groups of labeled samples. The items present in the groups are homogeneous in nature. Now, suppose we have an unlabeled example which needs to be classified into one of the several labeled groups. How do you do that? Unhesitatingly, using kNN Algorithm.

k nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups.

####kNN Algorithm features:
1. A very simple classification and regression algorithm.
      * In case of classification, new data points get classified in a particular class.
      * In case of regression, new data get labeled based on the avg. value of k nearest neighbour.

2. It is a lazy learner because it doesn't learn much from the training data.
3. It is supervised learning algorithm.
4. Default method is Euclidean distance (shortest distance between 2 points, using formula  = $\sqrt((X_1-X_2)^2+(Y_1-Y_2)^2)$ ) .

####Requirements for kNN
1. **Generally k gets decided on the square root of data points**.But a large k value has benefits which include reducing the variance due to the noisy data; the side effect being developing a bias due to which the learner tends to ignore the smaller patterns which may have useful insights
2. **Data Normalization** - It is to transform all the feature data in the same scale (for eg: 0 to 1) else it will give more weitage to the data which is higher in value irrespective of scale/unit.
3. Installation of "Class" library to implement in R.

####kNN Algorithm - Pros and Cons

**Pros**: The algorithm is highly unbiased in nature and makes no prior assumption of the underlying data. Being simple and effective in nature, it is easy to implement and has gained good popularity.

**Cons**: Indeed it is simple but kNN algorithm has drawn a lot of flake for being extremely simple! If we take a deeper look, this doesn't create a model since there's no abstraction process involved. Yes, the training process is really fast as the data is stored verbatim (hence lazy learner) but the prediction time is pretty high with useful insights missing at times. Therefore, building this algorithm requires time to be invested in data preparation (especially treating the missing data and categorical features) to obtain a robust model.


#### Steps Involved in performing kNN algorithm:
1. Data Collection.
2. Preparing and exploring the data.
      * Understnding data structure.
      * Data normalization.
      * Creating Training and Test data set.
3. Training a model on data.
4. Evaluate the model performance.
5. Improve the performance of model.


### Case Study on kNN algorithm

```{r}

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
gc <- read.csv("germancredit.csv")
head (gc)
## Taking back-up of the input file, in case the original data is required later

gc.bkup <- gc

## Convert the dependent var to factor. Normalize the numeric variables  

gc$Default <- factor(gc$Default)

num.vars <- sapply(gc, is.numeric)
gc[num.vars] <- lapply(gc[num.vars], scale)

## Selecting only 3 numeric variables for this demostration, just to keep things simple

myvars <- c("duration", "amount", "installment")
gc.subset <- gc[myvars]

summary(gc.subset)

## Let's predict on a test set of 100 observations. Rest to be used as train set.

set.seed(123) 
test <- 1:100
train.gc <- gc.subset[-test,]
test.gc <- gc.subset[test,]

train.def <- gc$Default[-test]
test.def <- gc$Default[test]

## Let's use k values (no of NNs) as 1, 5 and 20 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes as below...

library(class)

knn.1 <-  knn(train.gc, test.gc, train.def, k=1)
knn.5 <-  knn(train.gc, test.gc, train.def, k=5)
knn.20 <- knn(train.gc, test.gc, train.def, k=20)

## Let's calculate the proportion of correct classification for k = 1, 5 & 20 

100 * sum(test.def == knn.1)/100  # For knn = 1


100 * sum(test.def == knn.5)/100  # For knn = 5

100 * sum(test.def == knn.20)/100 # For knn = 20

## If we look at the above proportions, it's quite evident that K = 1 correctly classifies 68% of the outcomes, K = 5 correctly classifies 74% and K = 20 does it for 81% of the outcomes. 

## We should also look at the success rate against the value of increasing K.

table(knn.1 ,test.def)

## For K = 1, among 65 customers, 54 or 83%, is success rate. Let's look at k = 5 now

table(knn.5 ,test.def)

## For K = 5, among 76 customers, 63 or 82%, is success rate.Let's look at K = 20 now

table(knn.20 ,test.def)

##For K = 20, among 88 customers, 71 or 80%, is success rate.

## It seems increasing K increases the classification but reduces success rate. It is worse to class a customer as good when it is bad, than it is to class a customer as bad when it is good. 
## By looking at above success rates, K = 1 or K = 5 can be taken as optimum K.
## We can make a plot of the data with the training set in hollow shapes and the new ones filled in. 
## Plot for K = 1 can be created as follows - 

plot(train.gc[,c("amount","duration")],
     col=c(4,3,6,2)[gc.bkup[-test, "installment"]],
     pch=c(1,2)[as.numeric(train.def)],
     main="Predicted Default, by 1 Nearest Neighbors",cex.main=.95)

points(test.gc[,c("amount","duration")],
       bg=c(4,3,6,2)[gc.bkup[-test,"installment"]],
       pch=c(21,24)[as.numeric(knn.1)],cex=1.2,col=grey(.7))

legend("bottomright",pch=c(1,16,2,17),bg=c(1,1,1,1),
       legend=c("data 0","pred 0","data 1","pred 1"),
       title="default",bty="n",cex=.8)

legend("topleft",fill=c(4,3,6,2),legend=c(1,2,3,4),
       title="installment %", horiz=TRUE,bty="n",col=grey(.7),cex=.8)

## Plots are good way to represent data visually, but here it looks like an overkill as there are too many data on the plot.
